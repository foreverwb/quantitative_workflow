"""
CODE_AGGREGATOR - æ•°æ®èšåˆèŠ‚ç‚¹ï¼ˆé‡æ„ç‰ˆï¼‰
èŒè´£ï¼šä»…è´Ÿè´£å¤šæ¬¡ä¸Šä¼ æ•°æ®çš„å¢é‡åˆå¹¶
éªŒè¯å’Œè®¡ç®—ç”± Calculator èŠ‚ç‚¹è´Ÿè´£
"""

import json
from datetime import datetime
from typing import Dict, List, Tuple, Any
from pathlib import Path


def check_data_completeness(target: dict) -> dict:
    """
    æ£€æŸ¥åŸå§‹æ•°æ®å®Œæ•´æ€§ï¼ˆä¸ JSON Schema ä¿æŒä¸€è‡´ï¼‰
    
    æ£€æŸ¥å­—æ®µï¼š
    1. targets.symbol
    2. targets.spot_price
    3. targets.walls (4ä¸ªå­—æ®µ)
    4. targets.gamma_metrics (11ä¸ªå­—æ®µ)
    5. targets.directional_metrics (5ä¸ªå­—æ®µ)
    6. targets.atm_iv (3ä¸ªå­—æ®µ)
    
    æ€»è®¡ï¼š23ä¸ªåŸå§‹å­—æ®µï¼ˆä¸åŒ…æ‹¬è®¡ç®—å­—æ®µï¼‰
    
    Args:
        target: targets å­—å…¸
        
    Returns:
        {
            "is_complete": bool,
            "missing_fields": [],
            "total_required": 23,
            "provided": int
        }
    """
    missing_fields = []
    
    # 1. é¡¶å±‚å­—æ®µ
    if not is_valid_value(target.get("symbol")):
        missing_fields.append("symbol")
    if not is_valid_value(target.get("spot_price")):
        missing_fields.append("spot_price")
    
    # 2. walls (4ä¸ªå­—æ®µ)
    walls = target.get("walls", {})
    for field in ["call_wall", "put_wall", "major_wall", "major_wall_type"]:
        if not is_valid_value(walls.get(field)):
            missing_fields.append(f"walls.{field}")
    
    # 3. gamma_metrics (11ä¸ªå­—æ®µ)
    gamma = target.get("gamma_metrics", {})
    for field in ["vol_trigger", "spot_vs_trigger", "net_gex", 
                  "gap_distance_dollar"]:
        if not is_valid_value(gamma.get(field)):
            missing_fields.append(f"gamma_metrics.{field}")
    
    # æ£€æŸ¥ nearby_peak
    nearby_peak = gamma.get("nearby_peak", {})
    if not isinstance(nearby_peak, dict):
        missing_fields.append("gamma_metrics.nearby_peak")
    else:
        for field in ["price", "abs_gex"]:
            if not is_valid_value(nearby_peak.get(field)):
                missing_fields.append(f"gamma_metrics.nearby_peak.{field}")
    
    # æ£€æŸ¥ next_cluster_peak
    next_cluster = gamma.get("next_cluster_peak", {})
    if not isinstance(next_cluster, dict):
        missing_fields.append("gamma_metrics.next_cluster_peak")
    else:
        for field in ["price", "abs_gex"]:
            if not is_valid_value(next_cluster.get(field)):
                missing_fields.append(f"gamma_metrics.next_cluster_peak.{field}")
    
    # æ£€æŸ¥ monthly_dataï¼ˆå¯é€‰ï¼Œä½†å¦‚æœå­˜åœ¨éœ€è¦éªŒè¯ç»“æ„ï¼‰
    monthly_data = gamma.get("monthly_data", {})
    if monthly_data and isinstance(monthly_data, dict):
        monthly_cluster_strength = monthly_data.get("cluster_strength", {})
        if isinstance(monthly_cluster_strength, dict):
            # monthly_data å­˜åœ¨ä¸”ç»“æ„æ­£ç¡®ï¼Œç®—ä½œæœ‰æ•ˆ
            pass
    
    # æ£€æŸ¥ weekl_dataï¼ˆå¯é€‰ï¼Œä½†å¦‚æœå­˜åœ¨éœ€è¦éªŒè¯ç»“æ„ï¼‰
    weekly_data = gamma.get("weekly_data", {})
    if weekly_data and isinstance(weekly_data, dict):
        weekly_cluster_strength = weekly_data.get("cluster_strength", {})
        if isinstance(weekly_cluster_strength, dict):
            # weekly_data å­˜åœ¨ä¸”ç»“æ„æ­£ç¡®ï¼Œç®—ä½œæœ‰æ•ˆ
            pass
    
    # 4. directional_metrics (5ä¸ªå­—æ®µ)
    directional = target.get("directional_metrics", {})
    for field in ["dex_same_dir_pct", "vanna_dir", "vanna_confidence", 
                  "iv_path", "iv_path_confidence"]:
        if not is_valid_value(directional.get(field)):
            missing_fields.append(f"directional_metrics.{field}")
    
    # 5. atm_iv (3ä¸ªå­—æ®µ)
    atm_iv = target.get("atm_iv", {})
    for field in ["iv_7d", "iv_14d", "iv_source"]:
        if not is_valid_value(atm_iv.get(field)):
            missing_fields.append(f"atm_iv.{field}")
    
    total_required = 23  # 23ä¸ªåŸå§‹å­—æ®µ
    provided = total_required - len(missing_fields)
    
    return {
        "is_complete": len(missing_fields) == 0,
        "missing_fields": missing_fields,
        "total_required": total_required,
        "provided": provided,
        "completion_rate": int((provided / total_required) * 100)
    }


def smart_merge(first_data: dict, new_data: dict) -> Tuple[dict, dict]:
    """
    æ™ºèƒ½å¢é‡åˆå¹¶
    
    Args:
        first_data: å†å²æ•°æ®
        new_data: æ–°æ•°æ®
        
    Returns:
        (merged_data, merge_info)
    """
    merged = first_data.copy()
    
    # æå– targets
    first_targets = get_target_dict(first_data)
    new_targets = get_target_dict(new_data)
    
    # æ£€æµ‹æ–°æ•°æ®æ˜¯å¦ä¸ºç©º
    new_valid_count = count_valid_fields_in_dict(new_targets)
    
    if new_valid_count == 0:
        print("âš ï¸ è­¦å‘Š: æ–°æ•°æ®æ— æœ‰æ•ˆå­—æ®µ,è·³è¿‡åˆå¹¶")
        merge_info = {
            "new_fields_count": 0,
            "updated_fields_count": 0,
            "merge_failed": True,
            "failure_reason": "æ–°æ•°æ®æ— æœ‰æ•ˆå­—æ®µ"
        }
        return merged, merge_info
    
    # ç»Ÿè®¡ä¿¡æ¯
    new_fields_count = 0
    updated_fields_count = 0
    
    # åˆå¹¶å„ä¸ª section
    for section in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"]:
        if section not in first_targets:
            first_targets[section] = {}
        
        if section in new_targets:
            for key, new_value in new_targets[section].items():
                old_value = first_targets[section].get(key)
                
                if is_valid_value(new_value):
                    if not is_valid_value(old_value):
                        first_targets[section][key] = new_value
                        new_fields_count += 1
                    elif old_value != new_value:
                        first_targets[section][key] = new_value
                        updated_fields_count += 1
    
    # åˆå¹¶é¡¶å±‚å­—æ®µ
    for key in ["spot_price", "symbol"]:
        old_value = first_targets.get(key)
        new_value = new_targets.get(key)
        
        if is_valid_value(new_value):
            if not is_valid_value(old_value):
                first_targets[key] = new_value
                new_fields_count += 1
            elif old_value != new_value:
                first_targets[key] = new_value
                updated_fields_count += 1
    
    if new_fields_count == 0 and updated_fields_count == 0:
        print("âš ï¸ è­¦å‘Š: åˆå¹¶æœªäº§ç”Ÿä»»ä½•å˜åŒ–")
        merge_info = {
            "new_fields_count": 0,
            "updated_fields_count": 0,
            "merge_failed": True,
            "failure_reason": "æ— æ–°å¢æˆ–æ›´æ–°å­—æ®µ"
        }
        return merged, merge_info
    
    # æ›´æ–° targets
    merged["targets"] = first_targets
    
    merge_info = {
        "new_fields_count": new_fields_count,
        "updated_fields_count": updated_fields_count,
        "merge_failed": False
    }
    
    return merged, merge_info


def get_target_dict(data: Dict) -> Dict:
    """æå– targets å­—å…¸"""
    targets = data.get("targets")
    
    # ä¼˜å…ˆçº§1: ç›´æ¥æ˜¯å­—å…¸
    if isinstance(targets, dict) and targets:
        return targets
    
    # ä¼˜å…ˆçº§2: éç©ºåˆ—è¡¨
    if isinstance(targets, list) and targets:
        return targets[0] if isinstance(targets[0], dict) else {}
    
    # ä¼˜å…ˆçº§3: å›é€€åˆ°æ ¹èŠ‚ç‚¹
    if "spot_price" in data or "symbol" in data:
        print("âš ï¸ targetså­—æ®µç¼ºå¤±ï¼Œå°è¯•ä»æ ¹èŠ‚ç‚¹è¯»å–")
        return data
    
    print(f"âŒ æ— æ³•æå–targetsï¼Œç±»å‹: {type(targets)}")
    return {}


def is_valid_value(value: Any) -> bool:
    """åˆ¤æ–­å€¼æ˜¯å¦æœ‰æ•ˆï¼ˆéç¼ºå¤±å€¼ï¼‰"""
    if value is None:
        return False
    if value == -999:
        return False
    if value in ["N/A", "æ•°æ®ä¸è¶³", "", "unknown"]:
        return False
    return True


def count_valid_fields_in_dict(target_dict: dict) -> int:
    """ç»Ÿè®¡å­—å…¸ä¸­çš„æœ‰æ•ˆå­—æ®µæ•°é‡"""
    count = 0
    
    # æ ‡å‡†åµŒå¥—ç»“æ„
    for section in ["gamma_metrics", "directional_metrics", "atm_iv", "walls"]:
        if section in target_dict and isinstance(target_dict[section], dict):
            for value in target_dict[section].values():
                if is_valid_value(value):
                    count += 1
    
    # æ£€æŸ¥é¡¶å±‚å­—æ®µ
    for key in ["spot_price"]:
        if is_valid_value(target_dict.get(key)):
            count += 1
    
    return count


def extract_symbol(data: dict) -> str:
    """æå–è‚¡ç¥¨ä»£ç """
    target = get_target_dict(data)
    return target.get("symbol", data.get("symbol", "UNKNOWN"))


def format_merge_history(history: list) -> str:
    """æ ¼å¼åŒ–åˆå¹¶å†å²"""
    if not history:
        return "æ— å†å²è®°å½•"
    
    lines = []
    for record in history:
        lines.append(
            f"ç¬¬{record['round']}è½® ({record['timestamp']}): "
            f"{record['action']}, "
            f"æ–°å¢ {record.get('fields_added', 0)} ä¸ªå­—æ®µ"
        )
    return "\n".join(lines)


def main(
    agent3_output: dict,
    symbol: str,
    **env_vars
) -> dict:
    """
    æ•°æ®èšåˆèŠ‚ç‚¹å…¥å£ï¼ˆé‡æ„ç‰ˆï¼‰
    
    èŒè´£ï¼š
    1. åŠ è½½å†å²ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    2. åˆå¹¶æ–°æ—§æ•°æ®
    3. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å¹¶ç»™å‡ºæç¤º
    4. è¿”å›åˆå¹¶åçš„æ•°æ®
    
    Args:
        agent3_output: Agent3 è¾“å‡º
        symbol: è‚¡ç¥¨ä»£ç 
        **env_vars: ç¯å¢ƒå˜é‡
        
    Returns:
        {"result": åˆå¹¶åçš„æ•°æ® JSON å­—ç¬¦ä¸², "completeness": å®Œæ•´æ€§ä¿¡æ¯}
    """
    try:
        
        current_data = agent3_output
        
        # åŠ è½½å†å²ç¼“å­˜
        cache_dir = Path("data/temp")
        cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file = cache_dir / f"{symbol}_partial.json"
        
        if cache_file.exists():
            print(f"ğŸ“‚ å‘ç°å†å²ç¼“å­˜ï¼Œè¿›å…¥å¢é‡åˆå¹¶æ¨¡å¼")
            
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached = json.load(f)
            
            first_data = cached.get("data", {})
            merged_data, merge_info = smart_merge(first_data, current_data)
            
            # æ›´æ–°åˆå¹¶å†å²
            history = cached.get("merge_history", [])
            history.append({
                "round": len(history) + 1,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "fields_added": merge_info["new_fields_count"],
                "fields_updated": merge_info.get("updated_fields_count", 0),
                "action": "å¢é‡è¡¥é½" if not merge_info.get("merge_failed") else "åˆå¹¶å¤±è´¥",
                "failure_reason": merge_info.get("failure_reason", "")
            })
            merged_data["_merge_history"] = history
            
            merge_log = format_merge_history(history)
            
        else:
            print(f"âœ¨ é¦–æ¬¡è§£æï¼Œåˆå§‹åŒ–ç¼“å­˜")
            merged_data = current_data
            merge_history = [{
                "round": 1,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "fields_added": count_valid_fields_in_dict(get_target_dict(current_data)),
                "action": "é¦–æ¬¡è§£æ"
            }]
            merged_data["_merge_history"] = merge_history
            merge_log = format_merge_history(merge_history)
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        target = get_target_dict(merged_data)
        completeness = check_data_completeness(target)
        
        if not completeness["is_complete"]:
            missing_count = len(completeness["missing_fields"])
            print(f"âš ï¸ æ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå¤± {missing_count} ä¸ªå­—æ®µ:")
            for field in completeness["missing_fields"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   - {field}")
            if missing_count > 5:
                print(f"   ... è¿˜æœ‰ {missing_count - 5} ä¸ªå­—æ®µ")
            print(f"   å®Œæˆåº¦: {completeness['completion_rate']}% ({completeness['provided']}/{completeness['total_required']})")
        else:
            print(f"âœ… åŸå§‹æ•°æ®å®Œæ•´ ({completeness['provided']}/{completeness['total_required']} å­—æ®µ)")
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump({
                "symbol": symbol,
                "data": merged_data,
                "merge_history": merged_data.get("_merge_history", []),
                "completeness": completeness,
                "last_updated": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®èšåˆå®Œæˆï¼Œå·²ä¿å­˜åˆ° {cache_file}")
        
        # è¿”å›ç»“æœ
        return {
            "result": json.dumps(merged_data, ensure_ascii=False, indent=2),
            "merge_log": merge_log,
            "completeness": completeness,
            "symbol": symbol
        }
    
    except Exception as e:
        import traceback
        return {
            "result": json.dumps({
                "error": True,
                "error_message": str(e),
                "error_traceback": traceback.format_exc()
            }, ensure_ascii=False, indent=2)
        }